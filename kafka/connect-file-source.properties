#A logical name for your task
name=students-parquet

#Class of connector to use
connector.class=FileStreamSource

#Number of parallel tasks to launch. Provides scalability
tasks.max=1

#Local file to monitor for new input
file=/media/danielbastidas/HardDisk/BigData/source/students.parquet/part-00000-57b29883-068a-4173-a752-2e7d0168816c-c000.snappy.parquet

#Topic to publish data to.
topic=parquet-source

#value.converter=io.confluent.connect.avro.AvroConverter

#schema.registry.url=http://localhost:8081

#value.serializer=io.confluent.kafka.serializers.KafkaAvroSerializer
#value.schema='{"schema":"{\"namespace\": \"\",\"type\": \"record\",\"name\": \"StudentData\",\"fields\": [{\"name\": \"id\",\"type\": \"double\"}, {\"name\": \"female\",\"type\": \"string\"}, {\"name\": \"ses\",\"type\": \"string\"}, {\"name\": \"schtyp\",\"type\": \"string\"}, {\"name\": \"prog\",\"type\": \"string\"}, {\"name\": \"read\",\"type\": \"double\"}, {\"name\": \"write\",\"type\": \"double\"}, {\"name\": \"math\",\"type\": \"double\"}, {\"name\": \"science\",\"type\": \"double\"}, {\"name\": \"socst\",\"type\": \"double\"}, {\"name\": \"honors\",\"type\": \"string\"}, {\"name\": \"awards\",\"type\": \"double\"}, {\"name\": \"cid\",\"type\": \"int\"}]}"}'
#value.schema='{"type": "record","name": "StudentData","fields": [{"name": "id","type": "double"}, {"name": "female","type": "string"}, {"name": "ses","type": "string"}, {"name": "schtyp","type": "string"}, {"name": "prog","type": "string"}, {"name": "read","type": "double"}, {"name": "write","type": "double"}, {"name": "math","type": "double"}, {"name": "science","type": "double"}, {"name": "socst","type": "double"}, {"name": "honors","type": "string"}, {"name": "awards","type": "double"}, {"name": "cid","type": "int"}]}'

#"value.converter": "io.confluent.connect.avro.AvroConverter"
#"value.converter.schema.registry.url": "http://schema-registry:8081"

value.converter=io.confluent.connect.avro.AvroConverter
value.converter.schema.registry.url=http://localhost:8081
#value.converter.enhanced.avro.schema.support=true
#specific.avro.reader=false
#value.converter=org.apache.kafka.connect.json.JsonConverter
# connectors use converters instead of serializers
