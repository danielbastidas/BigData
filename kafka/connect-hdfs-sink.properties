#A logical name for your task
name=student-parquet-hdfs-sink

#Class of connector to use
#connector.class=io.confluent.connect.hdfs.HdfsSinkConnector
connector.class=io.confluent.connect.hdfs3.Hdfs3SinkConnector

#Topics to subscribe.
topics=parquet-source

#Number of parallel tasks to launch. Provides scalability
tasks.max=1

#HDFS URL to write the data to
#hdfs.url=hdfs://media/danielbastidas/HardDisk/BigData/source/sink
hdfs.url=hdfs://localhost:9000
topics.dir=/media/danielbastidas/HardDisk/BigData/sink
flush.size=3
confluent.topic.bootstrap.servers=localhost:9092
#value.converter=io.confluent.connect.avro.AvroConverter
#value.converter.schema.registry.url=http://schema-registry:8081
schemas.enable=true
enhanced.avro.schema.support=true
#value.schema='{"type": "record","name": "StudentData","fields": [{"name": "id","type": "double"}, {"name": "female","type": "string"}, {"name": "ses","type": "string"}, {"name": "schtyp","type": "string"}, {"name": "prog","type": "string"}, {"name": "read","type": "double"}, {"name": "write","type": "double"}, {"name": "math","type": "double"}, {"name": "science","type": "double"}, {"name": "socst","type": "double"}, {"name": "honors","type": "string"}, {"name": "awards","type": "double"}, {"name": "cid","type": "int"}]}'

key.converter=org.apache.kafka.connect.storage.StringConverter
#key.converter.schema.registry.url=http://localhost:8081
#key.converter.enhanced.avro.schema.support=true

value.converter=io.confluent.connect.avro.AvroConverter
value.converter.schema.registry.url=http://127.0.0.1:8081/
#value.converter.enhanced.avro.schema.support=true
value.deserializer=io.confluent.kafka.serializers.KafkaAvroDeSerializer
specific.avro.reader=true
schema.registry.url=http://localhost:8081/

# connectors use converters instead of serializers
