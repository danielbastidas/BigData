# start zookeeper
zookeeper-server-start.sh $KAFKA_HOME/config/zookeeper.properties

# start server
kafka-server-start.sh $KAFKA_HOME/config/server.properties

# start schema registry
bin/schema-registry-start etc/schema-registry/schema-registry.properties

# create the schema
curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" \
--data '{"schema":"{\"namespace\": \"\",\"type\": \"record\",\"name\": \"StudentData\",\"fields\": [{\"name\": \"id\",\"type\": \"double\"}, {\"name\": \"female\",\"type\": \"string\"}, {\"name\": \"ses\",\"type\": \"string\"}, {\"name\": \"schtyp\",\"type\": \"string\"}, {\"name\": \"prog\",\"type\": \"string\"}, {\"name\": \"read\",\"type\": \"double\"}, {\"name\": \"write\",\"type\": \"double\"}, {\"name\": \"math\",\"type\": \"double\"}, {\"name\": \"science\",\"type\": \"double\"}, {\"name\": \"socst\",\"type\": \"double\"}, {\"name\": \"honors\",\"type\": \"string\"}, {\"name\": \"awards\",\"type\": \"double\"}, {\"name\": \"cid\",\"type\": \"int\"}]}"}' \
http://localhost:8081/subjects/parquet-source-value/versions

# look created schemas
curl --silent -X GET http://localhost:8081/schemas/ids/1 | jq .

# producer/read data from the source
$KAFKA_HOME/bin/connect-standalone.sh $KAFKA_HOME/config/connect-standalone.properties /media/danielbastidas/HardDisk/BigData/kafka/connect-file-source.properties

# consumer/see what is in the topic
$KAFKA_HOME/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic parquet-source --from-beginning

# configure the kafka hadoop sink connector adding the following line in $KAFKA_HOME/config/connect-standalone.properties file 
plugin.path=/media/danielbastidas/HardDisk/BigData/kafka/confluentinc-kafka-connect-hdfs3-1.1.3/

# sending data to hadoop
$KAFKA_HOME/bin/connect-standalone.sh $KAFKA_HOME/config/connect-standalone.properties /media/danielbastidas/HardDisk/BigData/kafka/connect-hdfs-sink.properties

# format the hadoop file system
hdfs namenode -format

# hadoop temporary storage directory
/tmp/hadoop-danielbastidas/dfs/name

# Hadoop Start NameNode daemon and DataNode daemon: 
start-dfs.sh

# browse the hadoop app
http://localhost:9870/

# see the content in hadoop
hadoop fs -cat /media/danielbastidas/HardDisk/BigData/sink/parquet-source/partition=0/*
#hadoop fs -cat spark-data-engg/topics/file-source-test/partition=0/*

# create the hadoop folder
hadoop fs -mkdir /media/danielbastidas/HardDisk/BigData/sink

# list the hadoop file system
hadoop fs -ls

hadoop fs -ls /media/danielbastidas/HardDisk/BigData/sink/parquet-source/

# submit scala task to read data from hdfs
spark-submit --packages org.apache.spark:spark-avro_2.12:3.1.1 --class \
 "com.myscala.spark.HdfsTest" --master "local[*]" \
 "/home/danielbastidas/git-repo/Scala-Spark/build/libs/Scala-Spark-1.0.jar" \
 hdfs://localhost:9000/media/danielbastidas/HardDisk/BigData/sink/parquet-source/partition=0

#kafka create topic
kafka-topics.sh --zookeeper 127.0.0.1:2181 --topic first_topic --create --partitions 3 --replication-factor 1

# kafka list topics
kafka-topics.sh --zookeeper 127.0.0.1:2181 --list

# kafka describe topic
kafka-topics.sh --zookeeper 127.0.0.1:2181 --topic first_topic --describe

#kafka delete topic
kafka-topics.sh --zookeeper 127.0.0.1:2181 --topic first_topic --delete

# kafka console producer. The port is the kafka port
kafka-console-producer.sh --broker-list 127.0.0.1:9092 --topic first_topic

# kafka console producer properties
kafka-console-producer.sh --broker-list 127.0.0.1:9092 --topic first_topic --producer-property acks=all

# kafka console consumer. This command starts reading since this moment. If you want to read the whole topic execute the following command
kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 --topic first_topic

# kafka console consumer since the beginning
kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 --topic first_topic --from-beginning

# kafka console consumer group
kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 --topic first_topic --group my-first-application

# kafka list consumer groups
kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list

# kafka describe consumer group. Allows you to know which consumers are consuming data from the topic in real-time
kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-first-application

# kafka reset consumer group offset
kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group my-first-application --reset-offsets --to-earliest --execute --topic first_topic

# kafka reset consumer group offset to a certain number before or forward. In this case moving the offset to messages back
kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group my-first-application --reset-offsets --shift-by -2 --execute --topic first_topic

# persist the kafka messages so they are not deleted when the server is restarted
# edit the file $KAFKA_HOME/config/server.properties
log.dirs=/opt/kafka_2.13-2.8.0/data/kafka

# edit the file $KAFKA_HOME/config/zookeeper.properties
dataDir=/opt/kafka_2.13-2.8.0/data/zookeeper



kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group connect-student-parquet-hdfs-sink --reset-offsets --to-earliest --execute --topic parquet-source

# delete schema
curl -X DELETE http://localhost:8081/subjects/parquet-source-value?permanent=true

# schema compatibility
curl -X PUT -H "Content-Type: application/vnd.schemaregistry.v1+json" --data '{"compatibility": "NONE"}' http://localhost:8081/config



 --property value.schema='{"type": "record","name": "StudentData","fields": [{"name": "id","type": "double"}, {"name": "female","type": "string"}, {"name": "ses","type": "string"}, {"name": "schtyp","type": "string"}, {"name": "prog","type": "string"}, {"name": "read","type": "double"}, {"name": "write","type": "double"}, {"name": "math","type": "double"}, {"name": "science","type": "double"}, {"name": "socst","type": "double"}, {"name": "honors","type": "string"}, {"name": "awards","type": "double"}, {"name": "cid","type": "int"}]}'





$KAFKA_HOME/bin/connect-standalone.sh \
$KAFKA_HOME/config/connect-standalone.properties \
/media/danielbastidas/HardDisk/BigData/kafka/connect-file-source.properties \
--property value.serializer=io.confluent.kafka.serializers.KafkaAvroSerializer \
--property value.schema='{"type": "record","name": "StudentData","fields": [{"name": "id","type": "double"}, {"name": "female","type": "string"}, {"name": "ses","type": "string"}, {"name": "schtyp","type": "string"}, {"name": "prog","type": "string"}, {"name": "read","type": "double"}, {"name": "write","type": "double"}, {"name": "math","type": "double"}, {"name": "science","type": "double"}, {"name": "socst","type": "double"}, {"name": "honors","type": "string"}, {"name": "awards","type": "double"}, {"name": "cid","type": "int"}]}'



